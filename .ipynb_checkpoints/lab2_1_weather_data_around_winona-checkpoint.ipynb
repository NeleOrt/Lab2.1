{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxJucrKj7mHJ"
   },
   "source": [
    "# Lab 2.1 - Weather Data Around Winona\n",
    "\n",
    "In this lab, we will download and combine a decades worth of weather data from the NOAA, focusing on weather stations within 500 miles of Winona.\n",
    "\n",
    "Here is the outline of the basic process.\n",
    "\n",
    "1. Install and investigate useful packages.\n",
    "2. Find all weather stations in proximity to Winona.\n",
    "3. Use a single station to prototype our tools.\n",
    "4. Automate the process of downloading and uncompressing data from all stations of interest.\n",
    "5. Output the results to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeicFuT-8Vnx"
   },
   "source": [
    "## Problem 1 - Install and investigate useful tools.\n",
    "\n",
    "First, you should install and investigate the following tools.\n",
    "\n",
    "1. **`wget`** is a tool for programmically downloading data files from the web on the command line.  There is a Python wrapper to this tool that you can install with `pip` as shown below.\n",
    "2. **`geopy`** is a package that, among other things, implements a function for computing distances between two lat-long pairs. Again, install this package with `pip` as shown below.\n",
    "3. **`gzip`** is part of the standard Python library and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "c8pRv9PyCvPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\su6123ck\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "uPWEksjS9REC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\users\\su6123ck\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\su6123ck\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (from geopy) (2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQXgqoPb9e53"
   },
   "source": [
    "#### Task 1.1 - Investigate using `wget` to download a file.\n",
    "\n",
    "Read the help/documentation on `wget` to figure out how to download the following data file [Some random data file from STAT 210] into the `./data` sub-folder.\n",
    "\n",
    "[https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv](https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "D92NkCq_995Z"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "url ='https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv'\n",
    "file_path = wget.download(url, out='./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTW9hZXX-Ceq"
   },
   "source": [
    "#### Task 1.2 - Investigate using `geopy.distance.distance` to compute a distance in miles.\n",
    "\n",
    "1. Import the `distance` function from the `geopy.distance` submodule.\n",
    "2. Use Wikipedia to find the lat-long coordinates of Winona and Rochester MN.\n",
    "3. Use `distance` to compute the distance between Winona and Rochester.\n",
    "4. Use some other source (e.g., Google Maps) to check the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "m-XZZQ2v-i4t"
   },
   "outputs": [],
   "source": [
    "from geopy.distance import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "winona_coords = (44.0554, -91.6664)\n",
    "rochester_coords = (44.0121, -92.4802)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.64494286306752"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(distance_miles := distance(winona_coords, rochester_coords).miles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHdoE7ZFBXlU"
   },
   "source": [
    "#### Task 1.3 - Investigate `gzip`\n",
    "\n",
    "The yearly NOAA data is compressed as `.gz` files, which need to be uncompressed using `gzip`.  Explore the `gzip` module by\n",
    "\n",
    "1. Exploring the documentation/help for the `gzip` module,\n",
    "2. Using `wget` to download the following link into the `./data` folder, and\n",
    "3. Using `gzip` to uncompress this file.\n",
    "4. Inspect the data in your list, which should be of type `byte`.  Use a comprehension with the expression `l.decode('utf-8')` to convert this to a list of strings.\n",
    "5. Write the uncompressed lines to an output file using `with open(path, 'w') as out` and the `writelines` method of `out`.  \n",
    "\n",
    "**Link.** [https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz](https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "88NVuihyCBBO"
   },
   "outputs": [],
   "source": [
    "weather_url ='https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz'\n",
    "weather_path = wget.download(weather_url, out='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'ASN00002061,17500201,PRCP,56,,,a,\\n',\n",
       " b'ASN00003014,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00003059,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00003088,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00009015,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00009193,17500201,TMIN,187,,,a,\\n',\n",
       " b'ASN00009193,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00009500,17500201,DATX,2,,,a,\\n',\n",
       " b'ASN00009500,17500201,MDTX,210,,,a,\\n',\n",
       " b'ASN00009592,17500201,DATX,4,,,a,\\n']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open(weather_path, 'rb') as f_in:\n",
    "    uncompressed_data = f_in.readlines()\n",
    "uncompressed_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASN00002061,17500201,PRCP,56,,,a,\\n',\n",
       " 'ASN00003014,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00003059,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00003088,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00009015,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00009193,17500201,TMIN,187,,,a,\\n',\n",
       " 'ASN00009193,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00009500,17500201,DATX,2,,,a,\\n',\n",
       " 'ASN00009500,17500201,MDTX,210,,,a,\\n',\n",
       " 'ASN00009592,17500201,DATX,4,,,a,\\n']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weather_lines := [l.decode('utf-8') for l in uncompressed_data])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/weather_data.csv', 'w') as out:\n",
    "    out.writelines(weather_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt1MlIz9DWpm"
   },
   "source": [
    "## Problem 2 - Find all stations within 500 miles of Winona, MN.\n",
    "\n",
    "The file linked below contains information about all stations tracked by NOAA.  \n",
    "\n",
    "*Main folder:* https://www.ncei.noaa.gov/pub/data/ghcn/daily/\n",
    "\n",
    "*Station txt file:* https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\n",
    "\n",
    "*Note.* While it would be easier to use the CSV version of the station file, you should use the TXT version here (for practice).\n",
    "\n",
    "**Your tasks** Our goal is to get a list of stations that are within 500 miles of Winona.  Do this by\n",
    "\n",
    "1. Using `wget` to download the stations information into the `./data` folder.\n",
    "2. Use `with` to read the lines of this file.\n",
    "3. At this point, the lines are strings in a fixed-width format separated by whitespace.  Use a list comprehension with the string split method to split the raw lines (strings) into a list of entries.\n",
    "4. There are three entries of interest, the station ID and the lat-long coordinates of the station.  Inspect the file to determine the index for these three entries.\n",
    "5. We want to transform the lines (currently a list of strings) into a record, which is a `dict` with good names for the entries as keys and the values representing the data in an appropriate type (string for station ID, `float` for the lat-long).  Use a comprehension to create a list of records as described.\n",
    "6. Use another comprehension to apply a filter to the stations, keeping only those within 500 miles of Winona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_url ='https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "station_path = wget.download(station_url, out='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "F0J443KoFTrs",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       \\n',\n",
       " 'ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    \\n',\n",
       " 'AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196\\n',\n",
       " 'AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194\\n',\n",
       " 'AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217\\n',\n",
       " 'AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218\\n',\n",
       " 'AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930\\n',\n",
       " 'AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938\\n',\n",
       " 'AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948\\n',\n",
       " 'AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990\\n']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(station_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ACW00011604',\n",
       "  '17.1167',\n",
       "  '-61.7833',\n",
       "  '10.1',\n",
       "  'ST',\n",
       "  'JOHNS',\n",
       "  'COOLIDGE',\n",
       "  'FLD'],\n",
       " ['ACW00011647', '17.1333', '-61.7833', '19.2', 'ST', 'JOHNS'],\n",
       " ['AE000041196',\n",
       "  '25.3330',\n",
       "  '55.5170',\n",
       "  '34.0',\n",
       "  'SHARJAH',\n",
       "  'INTER.',\n",
       "  'AIRP',\n",
       "  'GSN',\n",
       "  '41196'],\n",
       " ['AEM00041194', '25.2550', '55.3640', '10.4', 'DUBAI', 'INTL', '41194'],\n",
       " ['AEM00041217',\n",
       "  '24.4330',\n",
       "  '54.6510',\n",
       "  '26.8',\n",
       "  'ABU',\n",
       "  'DHABI',\n",
       "  'INTL',\n",
       "  '41217'],\n",
       " ['AEM00041218', '24.2620', '55.6090', '264.9', 'AL', 'AIN', 'INTL', '41218'],\n",
       " ['AF000040930',\n",
       "  '35.3170',\n",
       "  '69.0170',\n",
       "  '3366.0',\n",
       "  'NORTH-SALANG',\n",
       "  'GSN',\n",
       "  '40930'],\n",
       " ['AFM00040938', '34.2100', '62.2280', '977.2', 'HERAT', '40938'],\n",
       " ['AFM00040948', '34.5660', '69.2120', '1791.3', 'KABUL', 'INTL', '40948'],\n",
       " ['AFM00040990',\n",
       "  '31.5000',\n",
       "  '65.8500',\n",
       "  '1010.0',\n",
       "  'KANDAHAR',\n",
       "  'AIRPORT',\n",
       "  '40990']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(split_lines := [line.split() for line in lines if line.strip()])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'station_id': 'ACW00011604', 'latitude': 17.1167, 'longitude': -61.7833},\n",
       " {'station_id': 'ACW00011647', 'latitude': 17.1333, 'longitude': -61.7833},\n",
       " {'station_id': 'AE000041196', 'latitude': 25.333, 'longitude': 55.517},\n",
       " {'station_id': 'AEM00041194', 'latitude': 25.255, 'longitude': 55.364},\n",
       " {'station_id': 'AEM00041217', 'latitude': 24.433, 'longitude': 54.651},\n",
       " {'station_id': 'AEM00041218', 'latitude': 24.262, 'longitude': 55.609},\n",
       " {'station_id': 'AF000040930', 'latitude': 35.317, 'longitude': 69.017},\n",
       " {'station_id': 'AFM00040938', 'latitude': 34.21, 'longitude': 62.228},\n",
       " {'station_id': 'AFM00040948', 'latitude': 34.566, 'longitude': 69.212},\n",
       " {'station_id': 'AFM00040990', 'latitude': 31.5, 'longitude': 65.85}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stations := [\n",
    "    {\n",
    "        'station_id': parts[0],  \n",
    "        'latitude': float(parts[1]),  \n",
    "        'longitude': float(parts[2])\n",
    "    }\n",
    "    for parts in split_lines\n",
    "])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'station_id': 'US1MNHS0001', 'latitude': 43.835, 'longitude': -91.314},\n",
       " {'station_id': 'US1MNHS0006', 'latitude': 43.742, 'longitude': -91.4369},\n",
       " {'station_id': 'US1MNHS0007', 'latitude': 43.8349, 'longitude': -91.3138},\n",
       " {'station_id': 'US1MNHS0008', 'latitude': 43.8381, 'longitude': -91.3079},\n",
       " {'station_id': 'US1MNHS0009', 'latitude': 43.8387, 'longitude': -91.3044},\n",
       " {'station_id': 'US1MNHS0012', 'latitude': 43.8253, 'longitude': -91.3209},\n",
       " {'station_id': 'US1MNHS0013', 'latitude': 43.7817, 'longitude': -91.3882},\n",
       " {'station_id': 'US1MNHS0022', 'latitude': 43.7921, 'longitude': -91.5856},\n",
       " {'station_id': 'US1MNHS0023', 'latitude': 43.7122, 'longitude': -91.6541},\n",
       " {'station_id': 'US1MNOL0038', 'latitude': 44.0762, 'longitude': -92.0979}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nearby_stations := [\n",
    "    station for station in stations \n",
    "    if distance(winona_coords, (station['latitude'], station['longitude'])).miles <= 25])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(num_stations := len(nearby_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"./data/ghcnd-stations-reference.txt\"\n",
    "# with open(data_path, 'r') as file:\n",
    "#     lines = file.readlines()\n",
    "# split_lines = [line.split() for line in lines if line.strip()]\n",
    "# stations = [\n",
    "#     {\n",
    "#         'station_id': parts[0],\n",
    "#         'latitude': float(parts[1]),\n",
    "#         'longitude': float(parts[2])\n",
    "#     }\n",
    "#     for parts in split_lines\n",
    "# ]\n",
    "# nearby_stations = [\n",
    "#     station for station in stations\n",
    "#     if distance(winona_coords, (station['latitude'], station['longitude'])).miles <= 25\n",
    "# ]\n",
    "# (num_stations := len(nearby_stations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This code generated the required 78 nearby stations until the website changed the entries for this specific file on 2025-01-30 16:33 (as indicated on the website). Now, the output inlcudes only 63 stations. For reference, the \"old\" ghcnd-station file is included in the data folder, named \"./data/ghcnd-stations-reference.txt\". The above commented code demonstrates this for reference if needed.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7_9ve26IduT"
   },
   "source": [
    "#### Problem 3 - Prototype downloading and uncompressing a station file.\n",
    "\n",
    "Before we download and uncompress all the stations of interest, let's practice on one station file.\n",
    "\n",
    "\n",
    "1. Copy the url for some station and store is as a variable named `url`.\n",
    "2. Write `lambda` functions that extract each of the following from the station `url`: compressed file name, compressed file path (e.g., `./data/...`), and uncompressed file path (e.g., `./data/...`).\n",
    "3. Write a `lambda` function that extracts\n",
    "4. Use `wget` to download this stations data.\n",
    "5. Use `gzip` to uncompress the data.\n",
    "6. Write the data to out output file.\n",
    "\n",
    "Your code should have the following shape:\n",
    "\n",
    "```{Python}\n",
    "wget.download(...)\n",
    "with gzip.open(...) as f:\n",
    "    with open(..., 'w') as out:\n",
    "        f.readlines()\n",
    "        out.writelines(f)\n",
    "```\n",
    "\n",
    "You should be using your helper functions to, in part, fill in the `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "04YS60A1JciS"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/ACW00011604.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_filename = lambda url: url.split('/')[-1]  \n",
    "compressed_filepath = lambda url: f\"./data/{compressed_filename(url)}\" \n",
    "uncompressed_filepath = lambda url: compressed_filepath(url).replace('.gz', '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACW00011604.csv.gz'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_filename(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/ACW00011604.csv.gz'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_filepath(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/ACW00011604.csv'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompressed_filepath(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = \"ID,YEAR/MONTH/DAY,ELEMENT,DATA VALUE,M-FLAG,Q-FLAG,S-FLAG,OBS-TIME\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download(url, compressed_filepath(url))\n",
    "with gzip.open(compressed_filepath(url), 'rt') as f: \n",
    "    with open('./data/output_file_example_station.csv', 'w') as out: \n",
    "        out.write(headers)\n",
    "        lines = f.readlines()\n",
    "        out.writelines(lines) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzgFqv5VF38i"
   },
   "source": [
    "## Problem 4 - Build the station URLs and download the files.\n",
    "\n",
    "**Tasks.** Now you need to build urls for all stations of interest by\n",
    "\n",
    "1. Use a comprehension to extract the stations of interest into a list.\n",
    "2. Investigating the structure of the files stored in the `by_station` folder (see main folder link above).\n",
    "3. Use a comprehension and an `f` string to build a list of URLS for all stations of interest.\n",
    "4. Use `wget` to download the data for the stations of interest into the data folder.\n",
    "5. Use `gzip` to uncompress the files.\n",
    "6. Convert the `bytes` to `str` of format `utf-8`.\n",
    "7. Use the append mode `\"a\"` of `open` with `writelines` to append the data in each file to your output file.\n",
    "\n",
    "While we usually avoid using a `for` loop, we make an exception for code for lengthy IO.  To accomplish steps 4 & 5, use a `for` loop with the following shape.\n",
    "\n",
    "```{Python}\n",
    "for url in station_urls:\n",
    "    wget.download(...)\n",
    "    with gzip.open(...) as f:\n",
    "        with open(..., 'a') as out:\n",
    "            f.readlines()\n",
    "            ... # Convert lines to strings here\n",
    "            out.writelines(f)\n",
    "    print(f\"Downloaded and extracted the data for {url}\")\n",
    "```\n",
    "\n",
    "Note that the code inside the loop should resemble the code from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US1MNHS0001',\n",
       " 'US1MNHS0006',\n",
       " 'US1MNHS0007',\n",
       " 'US1MNHS0008',\n",
       " 'US1MNHS0009',\n",
       " 'US1MNHS0012',\n",
       " 'US1MNHS0013',\n",
       " 'US1MNHS0022',\n",
       " 'US1MNHS0023',\n",
       " 'US1MNOL0038']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stations_of_interest := [station[\"station_id\"] for station in nearby_stations])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "SOUt7rCBIZ6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0001.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0006.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0007.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0008.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0009.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0012.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0013.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0022.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0023.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNOL0038.csv.gz']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\"\n",
    "(station_urls := [f\"{base_url}{station}.csv.gz\" for station in stations_of_interest])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"./data/stations_of_interest_winona_weather_data.csv\"\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    out.write(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0001.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0006.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0007.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0008.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0009.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0012.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0013.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0022.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0023.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNOL0038.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNOL0991.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWB0015.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0002.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0003.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0004.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0005.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0006.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0007.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0009.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0011.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0013.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0019.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0021.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0023.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0024.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0026.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0027.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0029.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0031.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0033.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WIBF0002.csv.gz\n",
      "HTTP Error 404 for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WIBF0010.csv.gz: Not Found\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0003.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0010.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0011.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0018.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0022.csv.gz\n",
      "HTTP Error 404 for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0038.csv.gz: Not Found\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0002.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0003.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0004.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0005.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0008.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0010.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00210146.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00210559.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00213812.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00214418.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00215488.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00217184.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00217277.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00218951.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00219067.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00219072.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00219077.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00470124.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00472165.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00472992.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00472996.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00474366.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00478589.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USW00004956.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USW00014920.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.error\n",
    "\n",
    "for url in station_urls: \n",
    "    try:\n",
    "        compressed_path = compressed_filepath(url)\n",
    "        wget.download(url, compressed_path)\n",
    "        with gzip.open(compressed_path, 'rt', encoding='utf-8') as f:\n",
    "            with open(output_file, 'a', encoding='utf-8') as out:\n",
    "                lines = f.readlines()\n",
    "                out.writelines(lines)\n",
    "        print(f\"Downloaded and extracted the data for {url}\")\n",
    "    \n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(f\"HTTP Error {e.code} for {url}: {e.reason}\")\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"URL Error for {url}: {e.reason}\")\n",
    "    except gzip.BadGzipFile:\n",
    "        print(f\"Error: The file downloaded from {url} is not a valid gzip file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The downloading problem of the failed two files (station_id US1WIBF0010 and US1WILC0038) could be due to reporting issues/data availability within NOAA since those two files are also missing on their official website under the by_station section.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code not used (it was already provided in notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tnGWOC1xF9kT",
    "outputId": "9b060cfe-aa8e-4930-880c-b720a36fc911"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://my_fake_website.cool/A123456789'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_station = \"A123456789\"\n",
    "make_fake_url = lambda s: f\"https://my_fake_website.cool/{s}\"\n",
    "\n",
    "make_fake_url(fake_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RPvTJEJGYtc",
    "outputId": "da643b74-66a9-4b29-8cce-a679a2d9fa5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://my_fake_website.cool/A0',\n",
       " 'https://my_fake_website.cool/A1',\n",
       " 'https://my_fake_website.cool/A2',\n",
       " 'https://my_fake_website.cool/A3',\n",
       " 'https://my_fake_website.cool/A4',\n",
       " 'https://my_fake_website.cool/A5',\n",
       " 'https://my_fake_website.cool/A6',\n",
       " 'https://my_fake_website.cool/A7',\n",
       " 'https://my_fake_website.cool/A8',\n",
       " 'https://my_fake_website.cool/A9']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_fake_stations =[f'A{i}' for i in range(10)]\n",
    "\n",
    "(my_fake_urls := [make_fake_url(s) for s in my_fake_stations])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
